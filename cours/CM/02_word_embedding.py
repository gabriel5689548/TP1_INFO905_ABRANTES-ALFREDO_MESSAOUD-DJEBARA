import marimo

__generated_with = "0.19.2"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell
def _(mo):
    from pathlib import Path

    # Chemin relatif au fichier notebook
    _asset_dir = Path(__file__).parent / "asset"

    mo.vstack(
            [
                mo.md(
                    r"""
                        # Word Embedding (Repr√©sentation vectorielle des mots)
                    
                        **Word Embedding** est une repr√©sentation des mots qui permet √† des mots ayant un sens similaire d'avoir une repr√©sentation semblable. Il s'agit d'une m√©thode d'apprentissage non supervis√© sur un vaste corpus textuel, o√π le mod√®le apprend √† pr√©dire un mot √† partir de son contexte ou inversement. Une fois entra√Æn√©, cette m√©thode produit des repr√©sentations vectorielles o√π des mots proches dans cet espace √† haute dimension sont cens√©s √™tre s√©mantiquement similaires.
                    
                        Contrairement √† une simple assignation de vecteurs uniques par mot, les embeddings capturent des similarit√©s **s√©mantiques** ou **syntaxiques** bas√©es sur le corpus d'entra√Ænement. Les vecteurs d'embedding contiennent souvent des centaines de dimensions et identifient des relations nuanc√©es entre les mots.
                            """
                    ),
                mo.image(src=_asset_dir / "word_embed.png"),
                mo.md(
                    r"""
                        ---
                    
                        ## Couche d'Embedding (Embedding Layer)
                    
                        Une **couche d'embedding** en apprentissage machine permet de cr√©er des repr√©sentations vectorielles (embeddings) √† partir de s√©quences d'entr√©e. Elle associe des mots ou des indices entiers √† des vecteurs denses de nombres r√©els.
                    
                        ### Processus :
                        - En entr√©e : une s√©quence d'indices de mots (par exemple, des entiers correspondant √† des mots dans un vocabulaire).
                        - En sortie : un tenseur o√π chaque s√©quence garde sa longueur originale, mais chaque mot/entier est repr√©sent√© par un vecteur dense.
                    
                        Ces vecteurs capturent les relations **s√©mantiques** entre les mots. La dimensionnalit√© de ces vecteurs est un hyperparam√®tre que l'on peut ajuster selon la t√¢che.
                    
                        ---
                    
                        ### Utilisation basique
                    
                        La couche d'embedding agit uniquement comme une **table de correspondance**. Chaque index est associ√© √† un vecteur dense qui peut √™tre mis √† jour lors de l'entra√Ænement.
                            """
                    )
                ]
            )
    return


@app.cell
def _():
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.utils.data as data
    import pandas as pd
    import numpy as np
    import plotly.graph_objects as go
    return F, data, go, nn, np, pd, torch


@app.cell
def _(mo):
    # UI pour configurer la couche d'embedding de d√©monstration
    embedding_dim_slider = mo.ui.slider(
            start=2, stop=16, step=1, value=5,
            label="Dimension de l'embedding"
            )
    word_selector = mo.ui.dropdown(
            options={"hello": "hello", "world": "world"},
            value="hello",
            label="Mot √† visualiser"
            )
    mo.hstack([embedding_dim_slider, word_selector])
    return embedding_dim_slider, word_selector


@app.cell
def _(embedding_dim_slider, mo, nn, torch, word_selector):
    # Cr√©ation d'un dictionnaire qui associe chaque mot √† un index unique
    word_to_ix = {"hello": 0, "world": 1}

    # Initialisation de la couche d'embedding avec dimension configurable
    embeds = nn.Embedding(num_embeddings=2, embedding_dim=embedding_dim_slider.value)

    # Transformation du mot s√©lectionn√© en tenseur
    lookup_tensor = torch.tensor([word_to_ix[word_selector.value]], dtype=torch.long)

    # R√©cup√©ration de l'embedding correspondant au mot s√©lectionn√©
    selected_embed = embeds(lookup_tensor)

    mo.md(
        f"""
    **Configuration actuelle:**
    - Mot s√©lectionn√©: `{word_selector.value}` (index: {word_to_ix[word_selector.value]})
    - Dimension de l'embedding: {embedding_dim_slider.value}
    - Vecteur d'embedding: `{selected_embed.detach().numpy()}`
    """
        )
    return (embeds,)


@app.cell
def _(mo):
    mo.md(
        r"""
            Param√®tres d'entrainements
            """
        )
    return


@app.cell
def _(embeds, mo):
    params_list = [str(param.data) for param in embeds.parameters()]
    mo.md(
        f"""
    **Param√®tres d'entra√Ænement (matrice d'embedding):**
    ```
    {params_list[0]}
    ```
    """
        )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
            Juste pour le fun on d√©finie nos propres param√®tres
            """
        )
    return


@app.cell
def _(nn, torch):
    # Cr√©ation d'une nouvelle couche d'embedding pour d√©monstration
    embeds_custom = nn.Embedding(num_embeddings=2, embedding_dim=5)

    embedding_lookup = torch.tensor(
            [
                [1, 0, 0, 0, 1],
                [0, 1, 1, 1, 0],
                ], dtype=torch.float32
            )
    embeds_custom.weight = nn.Parameter(embedding_lookup)
    for param_custom in embeds_custom.parameters():
        print(param_custom)
    return (embeds_custom,)


@app.cell
def _(mo):
    mo.md(
        r"""
            Comme vous pouvez le constater, si je s√©lectionne l'index 0 ou 1, j'obtiens ma ligne embedding_lookup
            """
        )
    return


@app.cell
def _(embeds_custom, torch):
    print(embeds_custom(torch.tensor([0])))
    print(embeds_custom(torch.tensor([1])))
    return


@app.cell
def _(mo):
    mo.md(
        r"""
            ## GPT embedding
        
            Regardons le tout premier mod√®le GPT et voyons la taille de la couche d'embedding.
            """
        )
    return


@app.cell
def _():
    from transformers import GPT2Tokenizer, GPT2Model

    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    gpt_model = GPT2Model.from_pretrained("gpt2")

    inputs_gpt = tokenizer("Hello, my dog is cute", return_tensors="pt")

    print("vocab size", tokenizer.vocab_size)

    # expected Embedding(50257, 768)
    # 50257 = vocabulary size
    # 768 = number of features
    print("Embedding size", gpt_model.wte)
    print(inputs_gpt)
    return


@app.cell
def _(mo):
    mo.md(
        r"""
            [Regardont le code de ce GPT](https://github.com/huggingface/transformers/blob/v4.25.1/src/transformers/models/gpt2/modeling_gpt2.py#L667)
            """
        )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
            ## Entra√Æner la premi√®re couche d'embedding
        
            Dans cette section, nous allons entra√Æner notre premi√®re couche d'embedding sur des critiques de films en fran√ßais (dataset Allocin√©) !
            Pour commencer, nous entra√Ænerons uniquement cette couche sur les lettres composant les mots.
            """
        )
    return


@app.cell
def _():
    from datasets import load_dataset

    # Chargement du dataset Allocin√© (critiques de films en fran√ßais)
    # Le dataset contient des critiques avec des labels de sentiment (0: n√©gatif, 1: positif)
    allocine_dataset = load_dataset("allocine", split="train[:5000]")
    allocine_dataset
    return (allocine_dataset,)


@app.cell
def _(allocine_dataset, pd):
    # Convertir en DataFrame pour visualisation
    df = pd.DataFrame(allocine_dataset)
    # Compter les diff√©rentes valeurs de label (0: n√©gatif, 1: positif)
    df
    return


@app.cell
def _(allocine_dataset):
    # R√©cup√©rer les critiques (texte en fran√ßais)
    reviews = allocine_dataset["review"]
    return (reviews,)


@app.cell
def _(mo):
    mo.md(
        r"""
            Maintenant, nous allons cr√©er une s√©quence de lettres bas√©e sur des phrases.
            Par exemple :
        
            ```
            aba decides a
            ```
        
            produira:
        
            ```
            [
              ('a', 'b'),
              ('b', 'a'),
              ('a', ' '),
              (' ', 'd'),
              ('d', 'e'),
              ('e', 'c'),
              ('c', 'i'),
              ('i', 'd'),
              ('d', 'e'),
              ('e', 's'),
              ('s', ' '),
              (' ', 'a'),
            ]
            ```
            """
        )
    return


@app.cell
def _(reviews):
    import itertools as it
    import re

    def sliding_window(txt):
        # G√©n√®re des paires de caract√®res (bigrammes) √† partir du texte donn√©
        # Exemple : 'chat' -> ('c', 'h'), ('h', 'a'), ('a', 't')
        for i in range(len(txt) - 1):
            yield txt[i], txt[i + 1]

    window = []

    for title in reviews:
        # Nettoie chaque titre pour ne conserver que les lettres (a-z) et les chiffres (0-9)
        title = re.sub('[^a-zA-Z0-9]+', '', title.lower())

        # Applique la fonction sliding_window au titre nettoy√© et l'ajoute √† la liste `window`
        window.append(sliding_window(title))

    window = list(it.chain(*window))

    # Affiche le nombre total de paires (bigrammes) g√©n√©r√©es
    print(len(window))

    # Affiche les 5 premi√®res paires g√©n√©r√©es en guise d'exemple
    window[:5]
    return (window,)


@app.cell
def _(mo):
    mo.md(
        r"""
            Maintenant, effectuons un encodage one-hot de mani√®re √† ce qu'une lettre corresponde √† un identifiant (comme un identifiant dans une table SQL).
        
            ```
            {' ': 2,
             'a': 0,
             'b': 1,
             'c': 5,
             'd': 3,
             'e': 4,
             'g': 8,
             'i': 6,
             'l': 16,
             'm': 12,
             'n': 9,
             'o': 11,
             'r': 15,
             's': 7,
             't': 10,
             'u': 13,
             'y': 14}
            ```
            """
        )
    return


@app.cell
def _(np, pd, window):
    # Mapping lettre avec un ID
    mapping = {c: i for i, c in enumerate(pd.DataFrame(window)[0].unique())}
    # Id en entr√©e du mod√®le
    integers_in = np.array([mapping[w[0]] for w in window])
    # Id en sortie du mod√®le
    integers_out = np.array([mapping[w[1]] for w in window])

    print("Shape of input", integers_in.shape)
    print("Input example", integers_in[0], integers_out[0])
    print("Show generate mapping\n", mapping)
    return integers_in, integers_out, mapping


@app.cell
def _(mo):
    mo.md(
        r"""
            ### La classe Dataset
        
            La classe `Dataset` r√©sume les fonctionnalit√©s de base d'un jeu de donn√©es de mani√®re naturelle.
            Pour d√©finir un jeu de donn√©es dans PyTorch, il suffit d'impl√©menter deux fonctions principales : `__getitem__` et `__len__`.
        
            1. **`__getitem__`** : Cette fonction doit retourner le i-√®me √©chantillon du jeu de donn√©es.
            2. **`__len__`** : Cette fonction retourne la taille totale du jeu de donn√©es.
        
            Ces deux fonctions garantissent une structure coh√©rente et standardis√©e pour interagir avec vos donn√©es.
            """
        )
    return


@app.cell
def _(data, torch):
    from typing import List, Tuple

    class NextLetterDataset(data.Dataset):
        def __init__(self, _integers_in: List[int], _integers_out: List[int]):
            self.integers_in = _integers_in  # Stocke les donn√©es d'entr√©e
            self.integers_out = _integers_out  # Stocke les √©tiquettes de sortie

        def __len__(self):
            return len(self.integers_in)

        def __getitem__(self, idx) -> Tuple[torch.tensor, torch.tensor]:
            """
            Retourne le i-√®me √©chantillon et son √©tiquette √† partir du dataset.
            Les donn√©es et √©tiquettes sont converties en tenseurs PyTorch avant d'√™tre renvoy√©es.

            Args:
            - idx (int): L'index de l'√©chantillon √† r√©cup√©rer.

            Returns:
            - Tuple[torch.tensor, torch.tensor]: Une paire contenant :
                - Le tenseur repr√©sentant la lettre en entr√©e
                - Le tenseur repr√©sentant la lettre en sortie
            """
            data_point = self.integers_in[idx]
            data_label = self.integers_out[idx]
            return torch.tensor(data_point), torch.tensor(data_label, dtype=torch.int64)

    return (NextLetterDataset,)


@app.cell
def _(mo):
    from pathlib import Path as _Path

    _asset_path = _Path(__file__).parent / "asset"

    mo.vstack(
            [
                mo.md(
                    r"""
                        ## Construire le premier mod√®le d'embedding
                        Nous allons construire un r√©seau simple pour pr√©dire la lettre suivante.
                            """
                    ),
                mo.image(src=_asset_path / "next_letter_prediction.png")
                ]
            )
    return


@app.cell
def _(F, torch):
    class NextLetterPrediction(torch.nn.Module):
        def __init__(self, vocab_size, embedding_size):
            super(NextLetterPrediction, self).__init__()
            self.embedding = torch.nn.Embedding(vocab_size, embedding_size)
            self.fc = torch.nn.Linear(embedding_size, vocab_size)

        def forward(self, x):
            x = F.relu(self.embedding(x))
            x = self.fc(x)
            return x

    return (NextLetterPrediction,)


@app.cell
def _(mo):
    mo.md(
        r"""
            ### Visualisation des lettres avant l'entra√Ænement
            Visualisons les embeddings des lettres avant l'entra√Ænement du mod√®le.
            """
        )
    return


@app.cell
def _(NextLetterPrediction, embedding_size_model, mapping, mo):
    model = NextLetterPrediction(
            vocab_size=len(mapping),
            embedding_size=embedding_size_model.value
            )
    mo.md(
        f"""
    **Mod√®le cr√©√© avec:**
    - Taille du vocabulaire: {len(mapping)}
    - Dimension de l'embedding: {embedding_size_model.value}
    """
        )
    return (model,)


@app.cell
def _(go, mapping, model, np, torch):
    idx_to_calc = list(mapping.values())
    idx_to_calc = np.array([idx_to_calc]).T

    translator = {v: k for k, v in mapping.items()}
    preds = model.embedding(torch.tensor(idx_to_calc)).detach().numpy()

    # Cr√©er un graphique Plotly interactif
    fig_before = None
    fig_before = go.Figure()
    fig_before.add_trace(
            go.Scatter(
                    x=preds[:, 0, 0],
                    y=preds[:, 0, 1],
                    mode='text',
                    text=[translator[idx[0]] for idx in idx_to_calc],
                    textfont=dict(size=14),
                    hoverinfo='text',
                    hovertext=[f"Lettre: {translator[idx[0]]}<br>x: {preds[i, 0, 0]:.3f}<br>y: {preds[i, 0, 1]:.3f}"
                               for i, idx in enumerate(idx_to_calc)]
                    )
            )
    fig_before.update_layout(
            title="Embeddings avant entra√Ænement",
            xaxis_title="Dimension 1",
            yaxis_title="Dimension 2",
            height=500
            )
    fig_before
    return


@app.cell
def _(mo):
    mo.md(
        r"""
            ### Train loop
        
            Configurez les hyperparam√®tres d'entra√Ænement ci-dessous:
            """
        )
    return


@app.cell
def _(mo):
    # Sliders pour les hyperparam√®tres d'entra√Ænement
    epochs_slider = mo.ui.slider(
            start=1, stop=10, step=1, value=1,
            label="Nombre d'√©poques"
            )
    batch_size_slider = mo.ui.slider(
            start=32, stop=512, step=32, value=128,
            label="Taille du batch"
            )
    learning_rate_slider = mo.ui.slider(
            start=0.001, stop=0.1, step=0.001, value=0.01,
            label="Taux d'apprentissage"
            )
    embedding_size_model = mo.ui.slider(
            start=2, stop=32, step=1, value=2,
            label="Dimension embedding (mod√®le)"
            )

    mo.vstack(
            [
                mo.hstack([epochs_slider, batch_size_slider]),
                mo.hstack([learning_rate_slider, embedding_size_model])
                ]
            )
    return (
        batch_size_slider,
        embedding_size_model,
        epochs_slider,
        learning_rate_slider,
        )


@app.cell
def _(
        NextLetterDataset,
        batch_size_slider,
        data,
        integers_in,
        integers_out,
        learning_rate_slider,
        model,
        nn,
        ):
    # Initialisation du dataset dans le DataLoader avec taille de batch configurable
    dataset = NextLetterDataset(integers_in, integers_out)
    trainloader = data.DataLoader(dataset, batch_size=batch_size_slider.value, shuffle=True)

    # Fonction de perte CrossEntropyLoss pour classification multi-classes
    criterion = nn.CrossEntropyLoss()

    # Optimiseur AdamW avec taux d'apprentissage configurable
    import torch as torch_optim
    optimizer = torch_optim.optim.AdamW(model.parameters(), lr=learning_rate_slider.value)
    return criterion, optimizer, trainloader


@app.cell
def _(mo):
    run_button = mo.ui.run_button()
    run_button
    return (run_button,)


@app.cell
def _(
        criterion,
        epochs_slider,
        mo,
        model,
        optimizer,
        run_button,
        torch,
        trainloader,
        ):
    mo.stop(not run_button.value, mo.md("Click üëÜ to run this cell"))
    # V√©rifie si un GPU CUDA est disponible ; sinon, utilise le CPU.
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Charger le mod√®le sur le dispositif choisi (GPU ou CPU) en mode entra√Ænement.
    model_train = model.to(device)
    model_train.train()

    # Historique des pertes pour visualisation
    loss_history = []

    # Boucle principale d'entra√Ænement
    for epoch in range(epochs_slider.value):
        running_loss = 0.0
        epoch_loss = 0.0
        n_batches = 0

        for i_batch, batch in enumerate(trainloader, 0):
            inputs, labels = batch
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model_train(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            epoch_loss += loss.item()
            n_batches += 1

        # Enregistrer la perte moyenne par √©poque
        loss_history.append(epoch_loss / n_batches)

    mo.md(
        f"""
    **Entra√Ænement termin√©!**
    - Device: `{device}`
    - √âpoques: {epochs_slider.value}
    - Perte finale: {loss_history[-1]:.4f}
    """
        )
    return device, loss_history, model_train


@app.cell
def _(go, loss_history, mo):
    # Graphique de l'historique des pertes
    fig_loss = go.Figure()
    fig_loss.add_trace(
            go.Scatter(
                    x=list(range(1, len(loss_history) + 1)),
                    y=loss_history,
                    mode='lines+markers',
                    name='Perte',
                    line=dict(color='blue', width=2),
                    marker=dict(size=8)
                    )
            )
    fig_loss.update_layout(
            title="√âvolution de la perte pendant l'entra√Ænement",
            xaxis_title="√âpoque",
            yaxis_title="Perte moyenne",
            height=400
            )
    mo.vstack([mo.md("### Courbe d'apprentissage"), fig_loss])
    return


@app.cell
def _(device, go, mapping, mo, model_train, np, torch):
    idx_to_calc_after = list(mapping.values())
    idx_to_calc_after = np.array([idx_to_calc_after]).T

    translator_after = {v: k for k, v in mapping.items()}
    preds_after = model_train.embedding(torch.tensor(idx_to_calc_after).to(device)).cpu().detach().numpy()

    # Graphique Plotly interactif pour les embeddings apr√®s entra√Ænement
    fig_after = go.Figure()
    fig_after.add_trace(
            go.Scatter(
                    x=preds_after[:, 0, 0],
                    y=preds_after[:, 0, 1],
                    mode='text+markers',
                    text=[translator_after[idx[0]] for idx in idx_to_calc_after],
                    textfont=dict(size=14),
                    marker=dict(size=10, opacity=0.6),
                    hoverinfo='text',
                    hovertext=[
                        f"Lettre: {translator_after[idx[0]]}<br>x: {preds_after[i, 0, 0]:.3f}<br>y: {preds_after[i, 0, 1]:.3f}"
                        for i, idx in enumerate(idx_to_calc_after)]
                    )
            )
    fig_after.update_layout(
            title="Embeddings apr√®s entra√Ænement",
            xaxis_title="Dimension 1",
            yaxis_title="Dimension 2",
            height=500
            )
    mo.vstack(
            [
                mo.md("### Visualisation des embeddings appris"),
                fig_after
                ]
            )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
            ## Comparaison s√©mantique avec CamemBERT
        
            Utilisez les champs ci-dessous pour comparer la similarit√© s√©mantique entre deux mots fran√ßais.
            """
        )
    return


@app.cell
def _(mo):
    # UI pour la comparaison de mots
    word1_input = mo.ui.text(value="reine", label="Premier mot")
    word2_input = mo.ui.text(value="roi", label="Deuxi√®me mot")
    mo.hstack([word1_input, word2_input], justify="start")
    return word1_input, word2_input


@app.cell
def _(F, mo, torch, word1_input, word2_input):
    from transformers import CamembertModel, CamembertTokenizer

    # Chargement du mod√®le CamemBERT
    camembert_model = CamembertModel.from_pretrained("camembert-base")
    camembert_tokenizer = CamembertTokenizer.from_pretrained("camembert-base")

    # Comparaison des mots saisis par l'utilisateur
    word1 = word1_input.value.strip() or "reine"
    word2 = word2_input.value.strip() or "roi"

    def get_word_embedding(word, model, tokenizer):
        """Extrait l'embedding d'un mot en utilisant la moyenne des tokens (excluant [CLS] et [SEP])"""
        inputs = tokenizer(word, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
        # last_hidden_state: [batch, seq_len, hidden_dim]
        # On exclut le premier token ([CLS]) et le dernier ([SEP])
        # et on fait la moyenne des embeddings des tokens du mot
        hidden_states = outputs.last_hidden_state[0, 1:-1, :]  # Exclure [CLS] et [SEP]
        return hidden_states.mean(dim=0)  # Moyenne sur les tokens

    embed1 = get_word_embedding(word1, camembert_model, camembert_tokenizer)
    embed2 = get_word_embedding(word2, camembert_model, camembert_tokenizer)

    # Calcul de la similarit√© cosinus sur les embeddings moyenn√©s
    cosine_sim = F.cosine_similarity(embed1.unsqueeze(0), embed2.unsqueeze(0), dim=1)
    sim_value = cosine_sim.item()

    # Interpr√©tation du score (seuils ajust√©s pour les embeddings de tokens)
    if sim_value > 0.85:
        interpretation = "Tr√®s similaires"
    elif sim_value > 0.7:
        interpretation = "Assez similaires"
    elif sim_value > 0.5:
        interpretation = "Mod√©r√©ment similaires"
    else:
        interpretation = "Peu similaires"

    mo.md(
        f"""
    ### R√©sultat de la comparaison

    | Mot 1 | Mot 2 | Similarit√© cosinus | Interpr√©tation |
    |-------|-------|-------------------|----------------|
    | **{word1}** | **{word2}** | **{sim_value:.4f}** | {interpretation} |

    > **Conseil:** Essayez des paires comme "chat/chien", "paris/france", "heureux/triste", "manger/boire"
    """
        )
    return


if __name__ == "__main__":
    app.run()
